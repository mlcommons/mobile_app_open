# Result format

This file explains the full format for results generated by this app.

This format is used when saving results locally on device, when uploading them to the online database,
and when fetching them from the online database.

## General structure

Results are serialized as JSON.

Results must be a map with the following items in the root level:

* `meta`: map
  * `uuid`: string  
  UUID is generated by the app when all benchmarks are finished.
  * `upload_date`: string
  Datetime of the moment when web service received the upload request
  Format is Iso 8601 in UTC timezone: `2022-04-14T03:54:54.687Z`
* `results`: list of maps. See [List of benchmark-specific results](#benchmark-specific-results) section
* `environment_info`: map. See [Environment info](#environment-info) section
* `build_info`: map. See [Build info](#application-build-info) section

## Benchmark-specific results

Each benchmark generates one map with results.

For example, if you select 3 benchmarks, you will get a list with 3 items, each describing specific benchmark.

Almost all of the settings are the same for performance and accuracy runs so they are united.

If Submission mode is disabled, `accuracy_run` will be null for all results.
If you enable Submission mode, both `performance_run` and `accuracy_run` values will be filled.

* `benchmark_id`: string
* `benchmark_name`: string  
  Value from `task.model.name` for this benchmark from selected tasks.pbtxt file.
* `loadgen_scenario`: string enum  
  See [`::mlperf::TestScenario`](https://github.com/mlcommons/inference/blob/a67f9f34bcc4439af4740095958c23380f9b284b/loadgen/test_settings.h#L38).  
  Allowed values:
  * `SingleStream`
  * `Offline`
* `backend_settings`: map  
  Settings defined by selected backend for this benchmark.
  * `accelerator_code`: string
  * `accelerator_desc`: string
  * `configuration`: string
  * `model_path`: string
  * `batch_size`: integer number
  * `extra_settings`: list of maps  
    Extra settings that can vary between different benchmarks and backends.  
    Here must be stored values set by backend in `common_setting`.  
    `shards_num` value for TFLite backend should be located here.  
    Map structure:
    * `id`: string. Value from `setting.id` that is passed to backend
    * `name`: string. Value from `setting.name` that is passed to backend
    * `value`: string. Value from `setting.value.value` that is passed to backend
    * `value_name`: string. Value from `setting.value.name` that is passed to backend
* `performance_run`: map  
  May be null if performance was not tested in this benchmark.
  * `throughput`: floating point number  
    Throughput value for this run of the benchmark.
    May be null for an accuracy run.
  * `accuracy`: floating point number  
    Accuracy value for this run of the benchmark.
    Value must be normalized between `0` and `100`.
    May be null for a performance run if groundtruth file is not provided.
  * `measured_duration_ms`: floating point number  
    Actual duration of the benchmark in milliseconds for start to finish.
  * `measured_samples`: integer number  
    Actual number of samples evaluated during the benchmark
  * `loadgen_validity`: bool  
    Value extracted from loadgen logs. Indicated whether all constraints were satisfied or not.
  * `start_datetime`: string  
    Datetime of the moment when benchmark started  
    Format is Iso 8601 in UTC timezone: `2022-04-14T03:54:54.687Z`
  * `dataset`: map  
    Dataset info for this benchmark from selected `tasks.pbtxt` file.
    * `name`: string
    * `type`: string enum  
      Allowed values (this list may be extended when we add support for more datasets):
      * `IMAGENET`
      * `COCO`
      * `ADE20K`
      * `SQUAD`
    * `data_path`: string
    * `groundtruth_path`: string
* `accuracy_run`: map  
  Same as `performance_run`.
  May be null if accuracy was not tested in this benchmark.
* `min_duration_ms`: floating point number  
  Value from `task.min_duration_ms` for this benchmark from selected tasks.pbtxt file.
* `min_samples`: integer number  
  Value from `task.min_query_count` for this benchmark from selected tasks.pbtxt file.
* `backend_info`: map
  * `filename`: string  
    Actual filename of the backend
  * `backend_name`: string  
    Backend name reported by backend
  * `vendor_name`: string  
    Vendor name reported by backend
  * `accelerator_name`: string  
    Backend-defined string describing actual accelerator used during this benchmark.  
    Should typically match `accelerator_desc` from the `backend_settings` map but may be different in case of accelerator fallback.

## Environment info

Info about environment the app is running in. May change when you update your OS, change device hardware, or use another device.

* `os`: string enum  
  Allowed values:
  * `android`
  * `ios`
  * `windows`
* `os_version`: string  
  Must be obtained from environment
* `manufacturer`: string. Manufacturer of the device  
  On Windows systems value must be `Unknown`.  
  On iOS systems value must be `Apple`.  
  On Android systems value must represent actual manufacturer name obtained from system environment.
* `model`: string. Manufacturer-defined model name  
  On Windows systems value must be `Unknown PC`.  
  On iOS systems value must be human-readable iPhone name. For example: `iPhone SE 2nd Gen`.  
  On Android systems value must represent actual model name obtained from system environment.

## Application build info

Constant info for this build of the app. The only way to change it is to use a different version of the app.

* `version`: string
* `build_number`: string
* `official_release_flag`: bool  
  Indicates if the official release flag was set for this build
* `dev_test_flag`: bool  
  Indicated if development test flag was set for this build
* `backend_list`: list of strings  
  Must contain actual list of backends that are included into this version of the app.
* `git_branch`: string
* `git_commit`: string
* `git_dirty_flag`: bool  
  Indicates if there are any local changes compared to the commit specified in `git_commit`.
