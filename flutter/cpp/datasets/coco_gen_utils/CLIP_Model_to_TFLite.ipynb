{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP model to TFLite\n",
        "\n",
        "Convert a CLIP model to TFLite with a resizing layer, so the CLIP model can process input image of size 512x512 instead of 224x224.\n",
        "\n",
        "You may need to use a high RAM instance when running this notebook.\n",
        "\n",
        "Based originally on this notebook:\n",
        "https://github.com/freedomtan/clip_score_on_android/blob/main/test_clip_model.ipynb"
      ],
      "metadata": {
        "id": "OgdL1j_-8DBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load CLIP model and processor"
      ],
      "metadata": {
        "id": "xtI21u-a9Ecg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAVED_MODEL_DIR = './clip_model'\n",
        "TFLITE_MODEL_PATH = './clip_model.tflite'\n",
        "MODEL_NAME = \"openai/clip-vit-large-patch14\""
      ],
      "metadata": {
        "id": "eOxB3zL_33tq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "\n",
        "from transformers import TFCLIPModel, CLIPProcessor\n",
        "\n",
        "# Load the pre-trained CLIP model and processor\n",
        "model = TFCLIPModel.from_pretrained(MODEL_NAME)\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Load the image from the URL\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SVKaEvWXPHY",
        "outputId": "750acf73-be71-4811-97f8-4f670da883e0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFCLIPModel.\n",
            "\n",
            "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the inputs\n",
        "inputs = processor(\n",
        "    text=[\"a photo of a cat\"],\n",
        "    images=image,\n",
        "    size={\"shortest_edge\": 512},\n",
        "    crop_size=512,\n",
        "    return_tensors=\"tf\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        ")\n",
        "for i in inputs:\n",
        "    print(i, \":\", inputs[i].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrZBfwE6fYAp",
        "outputId": "54b13a73-152c-4345-c016-2431edc06e4f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids : (1, 77)\n",
            "attention_mask : (1, 77)\n",
            "pixel_values : (1, 3, 512, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert CLIP model to TF SavedModel"
      ],
      "metadata": {
        "id": "-v1LmNLM9NXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new model that includes the resize operation\n",
        "class ResizedModel(tf.keras.Model):\n",
        "    def __init__(self, original_model):\n",
        "        super(ResizedModel, self).__init__()\n",
        "        self.original_model = original_model\n",
        "\n",
        "    def call(self, attention_mask, input_ids, pixel_values):\n",
        "        # Resize the pixel values to 224x224. pixel_values is expected to have NHWC layout.\n",
        "        resized_images = tf.image.resize(pixel_values, [224, 224])\n",
        "        # convert image from NHWC to NCHW\n",
        "        resized_images = tf.transpose(resized_images, [0, 3, 1, 2])\n",
        "        return self.original_model(\n",
        "            attention_mask=attention_mask,\n",
        "            input_ids=input_ids,\n",
        "            pixel_values=resized_images\n",
        "        )\n",
        "\n",
        "# Wrap the original model with the resize operation\n",
        "resized_model = ResizedModel(model)\n",
        "\n",
        "# Run the model\n",
        "outputs = resized_model(\n",
        "    inputs['attention_mask'],\n",
        "    inputs['input_ids'],\n",
        "    tf.transpose(inputs['pixel_values'], perm=[0, 2, 3, 1])\n",
        ")\n",
        "\n",
        "print('logits_per_image:', outputs['logits_per_image'])\n",
        "print('logits_per_text:', outputs['logits_per_text'])\n",
        "\n",
        "# Define a function that will be used as the signature to have named inputs when inspecting the model\n",
        "@tf.function(input_signature=[\n",
        "    tf.TensorSpec(shape=[None, 77], dtype=tf.int32, name='attention_mask'),\n",
        "    tf.TensorSpec(shape=[None, 77], dtype=tf.int32, name='input_ids'),\n",
        "    tf.TensorSpec(shape=[None, 512, 512, 3], dtype=tf.float32, name='pixel_values')\n",
        "])\n",
        "def serving_fn(attention_mask, input_ids, pixel_values):\n",
        "    output = resized_model(attention_mask, input_ids, pixel_values)\n",
        "    output_dict = {key: value for key, value in output.items() if isinstance(value, tf.Tensor)}\n",
        "    print(output_dict)\n",
        "    return output_dict\n",
        "\n",
        "# Save the model with the signature\n",
        "tf.saved_model.save(\n",
        "    resized_model,\n",
        "    SAVED_MODEL_DIR,\n",
        "    signatures={tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY: serving_fn}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP0-P6RzXYWU",
        "outputId": "745f4529-ef7f-44c5-cbc4-ca14a1e7b418"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits_per_image: tf.Tensor([[24.341438]], shape=(1, 1), dtype=float32)\n",
            "logits_per_text: tf.Tensor([[24.341438]], shape=(1, 1), dtype=float32)\n",
            "{'logits_per_image': <tf.Tensor 'resized_model_6/tfclip_model_2/clip/transpose:0' shape=(None, None) dtype=float32>, 'logits_per_text': <tf.Tensor 'resized_model_6/tfclip_model_2/clip/mul:0' shape=(None, None) dtype=float32>, 'text_embeds': <tf.Tensor 'resized_model_6/tfclip_model_2/clip/truediv_1:0' shape=(None, 512) dtype=float32>, 'image_embeds': <tf.Tensor 'resized_model_6/tfclip_model_2/clip/truediv:0' shape=(None, 512) dtype=float32>}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert TF SavedModel to TFLite model"
      ],
      "metadata": {
        "id": "hVgy1iQ39bnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "loaded = tf.saved_model.load(SAVED_MODEL_DIR)\n",
        "concrete_func = loaded.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
        "\n",
        "# Inspect the concrete function\n",
        "print(concrete_func.structured_input_signature)\n",
        "print(concrete_func.structured_outputs)\n",
        "\n",
        "# Convert the model to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converter.experimental_new_converter = True\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TensorFlow Lite model\n",
        "with open(TFLITE_MODEL_PATH, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgSyB3-CeTHh",
        "outputId": "d4ab1cc2-ede7-40ae-d308-ecfaac62bd93"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((), {'attention_mask': TensorSpec(shape=(None, 77), dtype=tf.int32, name='attention_mask'), 'pixel_values': TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name='pixel_values'), 'input_ids': TensorSpec(shape=(None, 77), dtype=tf.int32, name='input_ids')})\n",
            "{'logits_per_text': TensorSpec(shape=(None, None), dtype=tf.float32, name='logits_per_text'), 'image_embeds': TensorSpec(shape=(None, 512), dtype=tf.float32, name='image_embeds'), 'text_embeds': TensorSpec(shape=(None, 512), dtype=tf.float32, name='text_embeds'), 'logits_per_image': TensorSpec(shape=(None, None), dtype=tf.float32, name='logits_per_image')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the converted TFLite model"
      ],
      "metadata": {
        "id": "XPsN2OWC88O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorFlow Lite model\n",
        "i = tf.lite.Interpreter(TFLITE_MODEL_PATH)\n",
        "i.allocate_tensors()\n",
        "\n",
        "# Set the input tensors\n",
        "# convert image from NCHW to NHWC\n",
        "pixel_values = tf.transpose(inputs['pixel_values'], [0, 2, 3, 1])\n",
        "assert(pixel_values.shape == (1, 512, 512, 3))\n",
        "i.set_tensor(0, inputs['attention_mask'])\n",
        "i.set_tensor(1, inputs['input_ids'])\n",
        "i.set_tensor(2, pixel_values)\n",
        "\n",
        "# Run inference\n",
        "i.invoke()\n",
        "\n",
        "# Print the outputs of TFLite model\n",
        "print('TFLite model:')\n",
        "print(f'logits_per_image', i.get_tensor(i.get_output_details()[1]['index']))\n",
        "print(f'logits_per_text', i.get_tensor(i.get_output_details()[2]['index']))\n",
        "\n",
        "# Print the outputs of the original model for comparision\n",
        "print('Original model:')\n",
        "print('logits_per_image:', outputs['logits_per_image'].numpy())\n",
        "print('logits_per_text:', outputs['logits_per_text'].numpy())"
      ],
      "metadata": {
        "id": "MpdP69YykiIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1602e98f-5d0b-4c57-c4d3-319ef2ea749d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFLite model:\n",
            "logits_per_image [[24.341446]]\n",
            "logits_per_text [[24.341446]]\n",
            "Original model:\n",
            "logits_per_image: [[24.341438]]\n",
            "logits_per_text: [[24.341438]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MdQ6NfvsBeFP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}