{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP model to TFLite\n",
        "\n",
        "Convert a CLIP model to TFLite with a resizing layer, so the CLIP model can process input image of size 512x512 instead of 224x224.\n",
        "\n",
        "You may need to use a high RAM instance when running this notebook.\n",
        "\n",
        "Based originally on this notebook:\n",
        "https://github.com/freedomtan/clip_score_on_android/blob/main/test_clip_model.ipynb"
      ],
      "metadata": {
        "id": "OgdL1j_-8DBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load CLIP model and processor"
      ],
      "metadata": {
        "id": "xtI21u-a9Ecg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAVED_MODEL_DIR = './clip_model'\n",
        "TFLITE_MODEL_PATH = './clip_model.tflite'"
      ],
      "metadata": {
        "id": "eOxB3zL_33tq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "\n",
        "from transformers import TFCLIPModel, CLIPProcessor\n",
        "\n",
        "# Load the pre-trained CLIP model and processor\n",
        "model = TFCLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# Load the image from the URL\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SVKaEvWXPHY",
        "outputId": "5ade8180-4624-4b68-a312-af19eab22010"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFCLIPModel.\n",
            "\n",
            "All the weights of TFCLIPModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the inputs\n",
        "inputs = processor(\n",
        "    text=[\"a photo of a cat\"],\n",
        "    images=image,\n",
        "    size={\"shortest_edge\": 512},\n",
        "    crop_size=512,\n",
        "    return_tensors=\"tf\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        ")\n",
        "for i in inputs:\n",
        "    print(i, \":\", inputs[i].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrZBfwE6fYAp",
        "outputId": "efeab23b-ef99-4292-f1ee-967d3d2fdb2c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids : (1, 77)\n",
            "attention_mask : (1, 77)\n",
            "pixel_values : (1, 3, 512, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert CLIP model to TF SavedModel"
      ],
      "metadata": {
        "id": "-v1LmNLM9NXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new model that includes the resize operation\n",
        "class ResizedModel(tf.keras.Model):\n",
        "    def __init__(self, original_model):\n",
        "        super(ResizedModel, self).__init__()\n",
        "        self.original_model = original_model\n",
        "\n",
        "    def call(self, attention_mask, input_ids, pixel_values):\n",
        "        # Resize the pixel values to 224x224. pixel_values is expected to have NHWC layout.\n",
        "        resized_images = tf.image.resize(pixel_values, [224, 224])\n",
        "        # convert image from NHWC to NCHW\n",
        "        resized_images = tf.transpose(resized_images, [0, 3, 1, 2])\n",
        "        return self.original_model(\n",
        "            attention_mask=attention_mask,\n",
        "            input_ids=input_ids,\n",
        "            pixel_values=resized_images\n",
        "        )\n",
        "\n",
        "# Wrap the original model with the resize operation\n",
        "resized_model = ResizedModel(model)\n",
        "\n",
        "# Define a function that will be used as the signature to have named inputs when inspecting the model\n",
        "@tf.function(input_signature=[\n",
        "    tf.TensorSpec(shape=[None, 77], dtype=tf.int32, name='attention_mask'),\n",
        "    tf.TensorSpec(shape=[None, 77], dtype=tf.int32, name='input_ids'),\n",
        "    tf.TensorSpec(shape=[None, 512, 512, 3], dtype=tf.float32, name='pixel_values')\n",
        "])\n",
        "def serving_fn(attention_mask, input_ids, pixel_values):\n",
        "    output = resized_model(attention_mask, input_ids, pixel_values)\n",
        "    output_dict = {key: value for key, value in output.items() if isinstance(value, tf.Tensor)}\n",
        "    print(output_dict)\n",
        "    return output_dict\n",
        "\n",
        "# Save the model with the signature\n",
        "tf.saved_model.save(\n",
        "    resized_model,\n",
        "    SAVED_MODEL_DIR,\n",
        "    signatures={tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY: serving_fn}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP0-P6RzXYWU",
        "outputId": "44a41ccd-1507-4635-e2d9-86011de218bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'logits_per_image': <tf.Tensor 'resized_model/tfclip_model/clip/transpose:0' shape=(None, None) dtype=float32>, 'logits_per_text': <tf.Tensor 'resized_model/tfclip_model/clip/mul:0' shape=(None, None) dtype=float32>, 'text_embeds': <tf.Tensor 'resized_model/tfclip_model/clip/truediv_1:0' shape=(None, 768) dtype=float32>, 'image_embeds': <tf.Tensor 'resized_model/tfclip_model/clip/truediv:0' shape=(None, 768) dtype=float32>}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert TF SavedModel to TFLite model"
      ],
      "metadata": {
        "id": "hVgy1iQ39bnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "loaded = tf.saved_model.load(SAVED_MODEL_DIR)\n",
        "concrete_func = loaded.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
        "\n",
        "# Inspect the concrete function\n",
        "print(concrete_func.structured_input_signature)\n",
        "print(concrete_func.structured_outputs)\n",
        "\n",
        "# Convert the model to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converter.experimental_new_converter = True\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TensorFlow Lite model\n",
        "with open(TFLITE_MODEL_PATH, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgSyB3-CeTHh",
        "outputId": "a8b81df3-dbf0-43ad-ba7e-464ec25d2539"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((), {'attention_mask': TensorSpec(shape=(None, 77), dtype=tf.int32, name='attention_mask'), 'input_ids': TensorSpec(shape=(None, 77), dtype=tf.int32, name='input_ids'), 'pixel_values': TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name='pixel_values')})\n",
            "{'logits_per_image': TensorSpec(shape=(None, None), dtype=tf.float32, name='logits_per_image'), 'text_embeds': TensorSpec(shape=(None, 768), dtype=tf.float32, name='text_embeds'), 'image_embeds': TensorSpec(shape=(None, 768), dtype=tf.float32, name='image_embeds'), 'logits_per_text': TensorSpec(shape=(None, None), dtype=tf.float32, name='logits_per_text')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the converted TFLite model"
      ],
      "metadata": {
        "id": "XPsN2OWC88O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorFlow Lite model\n",
        "i = tf.lite.Interpreter(TFLITE_MODEL_PATH)\n",
        "i.allocate_tensors()\n",
        "\n",
        "# Set the input tensors\n",
        "# convert image from NCHW to NHWC\n",
        "pixel_values = tf.transpose(inputs['pixel_values'], [0, 2, 3, 1])\n",
        "assert(pixel_values.shape == (1, 512, 512, 3))\n",
        "i.set_tensor(0, inputs['attention_mask'])\n",
        "i.set_tensor(1, inputs['input_ids'])\n",
        "i.set_tensor(2, pixel_values)\n",
        "\n",
        "# Run inference\n",
        "i.invoke()\n",
        "\n",
        "# Print the outputs\n",
        "print(f'logits_per_image', i.get_tensor(i.get_output_details()[1]['index']))\n",
        "print(f'logits_per_text', i.get_tensor(i.get_output_details()[2]['index']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpdP69YykiIl",
        "outputId": "c5b5e65c-eb66-490a-ae10-c7e832574855"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits_per_image [[18.023521]]\n",
            "logits_per_text [[18.023521]]\n"
          ]
        }
      ]
    }
  ]
}